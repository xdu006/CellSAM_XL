{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "whPIj-vy8hQe",
        "iOGU8xv5AuQo"
      ],
      "mount_file_id": "10r4ST8sHYhQhn7_9aOdGgL8--AgFOwOG",
      "authorship_tag": "ABX9TyOPBci7y+XBnj3UHPwPTLi6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xdu006/CellSAM_XL/blob/main/CellSAM_XL_Gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Iu-9wpYqwHi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup\n",
        "\n",
        "!pip install gradio\n",
        "!pip install -q git+https://github.com/xdu006/cellSAM_test.git\n",
        "\n",
        "import cellSAM\n",
        "from cellSAM import segment_cellular_image, get_model\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy.ndimage import binary_dilation\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "import tifffile as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "P402ORcSwGE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FUNCTIONS"
      ],
      "metadata": {
        "id": "sGAj272rItEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "whPIj-vy8hQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizes all channels by converting each channel into a color in RGB format\n",
        "def visualize_channels(channels):\n",
        "\n",
        "  rgb: None | np.ndarray = None\n",
        "\n",
        "  #choose color pallet\n",
        "  colors = [(255, 0, 0), (0, 255, 0), (160,32,240), (0, 0, 255), (255, 0, 255), (0, 255, 255)]\n",
        "  #colors = [(255, 0, 0), (0, 255, 0), (255, 255, 0), (0, 0, 255), (255, 0, 255), (0, 255, 255)]\n",
        "  #colors = [(255, 255, 0), (255, 255, 255), (160,32,240), (0, 255, 255), (255, 0, 255), (0, 255, 0)]\n",
        "\n",
        "  #for every channel\n",
        "  for i,channel in enumerate(channels):\n",
        "\n",
        "    rgb_factor = colors[i] #set scaling factor/color for the channel\n",
        "\n",
        "    # Create base RGB-TIFF arrays\n",
        "    if rgb is None: rgb = np.zeros((channel.shape[0], channel.shape[1], 3), dtype=\"float64\")\n",
        "\n",
        "    rgb_multiplier = (np.array(rgb_factor,)/ 255) #use factor to create a multiplier\n",
        "\n",
        "    # Iterate over each channel index and multiplier in rgb_multipliers\n",
        "    for i, multiplier in enumerate(rgb_multiplier):\n",
        "      # Multiply the channel with the current multiplier\n",
        "      # Add the resulting channel_rgb to the corresponding channel in the rgb array\n",
        "      rgb[:, :, i] += (channel * multiplier).astype(\"float64\")\n",
        "\n",
        "    # Now the rgb array holds the accumulated values for each channel\n",
        "    # Scale and convert to float64 with intensity between 0 and 1\n",
        "    assert rgb is not None\n",
        "    rgb = np.clip(rgb, 0, 1).astype(\"float64\")\n",
        "\n",
        "  return rgb # return the RGB image for display"
      ],
      "metadata": {
        "id": "PqUdfQSw8hEG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Process Image"
      ],
      "metadata": {
        "id": "iOGU8xv5AuQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reads tiff files from given file name\n",
        "def read_and_enhance_file (file_name):\n",
        "\n",
        "  raw_tiff_file = tf.imread(file_name).astype(np.float64) #read file\n",
        "\n",
        "  #Check input file shape (ensure that channel is last)\n",
        "  match raw_tiff_file.ndim:\n",
        "    case 0: message = \"Loading image... No data detected, please check tiff file loaded\"            ### consider adding a case none for the ndim check...\n",
        "    case 1: message = \"Loading image... Image Loaded: File only had one dimention. Are you sure you loaded a 2D image...?\"\n",
        "    case 2: message = \"Loading image... Image Loaded: 2 dimension image detected.\"\n",
        "    case 3:\n",
        "      message = \"Image Loaded: 3 dimension image detected. Converted to HWC format.\"\n",
        "      if raw_tiff_file.shape[0] < raw_tiff_file.shape[1]: #always convert to # H, W, C\n",
        "        print(f\"Channel shape is {raw_tiff_file.shape}. Performing convertion to (H,W,C) format. \")\n",
        "        raw_tiff_file = np.transpose(raw_tiff_file, (1, 2, 0))\n",
        "        print(f\"Channel shape is now {raw_tiff_file.shape}.\")\n",
        "    case 4: message = \"Image Loaded: 4 dimension image detected... Currently we do not support 3D images.\"\n",
        "    case _: message = \"Image Loaded: Unknown image format.\"\n",
        "\n",
        "  #performe image enhancements\n",
        "  original_image = np.copy(raw_tiff_file) #copy to ensure that original data is unaltered\n",
        "  channel_scaled = np.copy(raw_tiff_file)\n",
        "  for c in range(raw_tiff_file.shape[2]): channel_scaled[:,:,c] = raw_tiff_file[:,:,c]/max(raw_tiff_file[:,:,c].flatten()) #channel dependent scaling based on maxiumin in each channel\n",
        "  scaled_gamma15 = np.copy(channel_scaled)\n",
        "  scaled_gamma15 = np.power(channel_scaled, 1/1.5) #gamma 1.5 scaling\n",
        "  raw_tiff_file =  scaled_gamma15 #\n",
        "\n",
        "  message = f\"Image loaded. Image shape {raw_tiff_file.shape}, in HWC format\" # success message\n",
        "  global processed_img\n",
        "  processed_img = raw_tiff_file # update global variable\n",
        "\n",
        "  return message, visualize_channels(np.transpose(raw_tiff_file, (2,0,1)))"
      ],
      "metadata": {
        "id": "9_fGmONqIrE_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "C6MoVjArAx0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Tiling helper functions\n",
        "\n",
        "#calculate approriate dimentions for cropping based on cell size\n",
        "def get_crop_dimentions(cell_size):\n",
        "  ICR = 17.63  #Target Image to Cell Size Ratio: KNOWN VALUE based on our experiments\n",
        "  return int(round(cell_size*ICR)) #ratio to crop\n",
        "\n",
        "#uses inputted crop dimentions to crop and return an list of tiles\n",
        "def get_tiles(im, yDim: int, xDim: int, saveFolder=None, id=None, ijmeta=None):\n",
        "  if im.shape[0] <= yDim or im.shape[1] <= xDim: return [im]\n",
        "  tiles = [im[y:y+yDim, x:x+xDim, :] for y in range(0,im.shape[0],yDim) for x in range(0,im.shape[1],xDim)]\n",
        "  for i in range(len(tiles)): #add padding to tiles outside of region\n",
        "    if tiles[i].shape[0] != yDim or tiles[i].shape[1] != xDim:\n",
        "      padding = np.zeros( shape=(yDim,xDim,im.shape[2]), dtype=np.float64)\n",
        "      padding[0:tiles[i].shape[0], 0:tiles[i].shape[1], :] = tiles[i]\n",
        "      tiles[i] = padding\n",
        "  return tiles\n",
        "\n",
        "\n",
        "### Data extraction helper functions\n",
        "\n",
        "#returns the number of cells represented by the size (dim1) of the bounding boxes list\n",
        "def get_num_cells(bb):\n",
        "  if bb is None: return 0\n",
        "  return bb.shape[0]\n",
        "\n",
        "#calculate and return centroids for each bounding box\n",
        "def get_centroids(bounding_boxes, img_scale_dim, return_scaled_BB=False):\n",
        "  centroids = []\n",
        "  if bounding_boxes is None: return centroids                                                                             # NOTE: consider using a set for better searching efficiency?\n",
        "\n",
        "  bb = bounding_boxes.cpu().detach().numpy()/1024*img_scale_dim   #x1, y1, x2, y2\n",
        "  for x1, y1, x2, y2 in bb: centroids.append( ((x1+x2)/2, (y1+y2)/2) )\n",
        "  if return_scaled_BB: return centroids, bb\n",
        "  return centroids"
      ],
      "metadata": {
        "id": "JEtiFsj9wV9E"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run CellSAM"
      ],
      "metadata": {
        "id": "d44Fz32sAntO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# runs cellSAM on specified channels for all tiles\n",
        "# compiles and returns and pandas dataframe with detailed extracted quantification data\n",
        "# as well as a summary dataframe with only summed cell counts per channel\n",
        "\n",
        "def Run_CellSAM (channels, cell_size):\n",
        "\n",
        "  #Crop file to appropriate dimentions\n",
        "  cd = get_crop_dimentions(cell_size)\n",
        "  tiles = get_tiles(im=processed_img, yDim=cd, xDim=cd)\n",
        "\n",
        "  #Ensure lists are expected format\n",
        "  print(f\"Dimentions for tiles: {cd}\")\n",
        "  if isinstance(tiles, list): print(f\"Tiles is a list of size {len(tiles)}!\")\n",
        "  else: tiles = [tiles]\n",
        "  if isinstance(channels, list): print(f\"Channels is a list of size {len(channels)}!\")\n",
        "  else: channels = [channels]\n",
        "\n",
        "  #initialize dataframe\n",
        "  SUMMARY_ARRAY = pd.DataFrame(columns=['Channel', 'Tile', 'Count', 'Centroid_Coords'])\n",
        "  cells_per_channel = []\n",
        "\n",
        "  for i in channels: #for each chosen channel\n",
        "\n",
        "    #run cellSAM on each tile\n",
        "    results = [segment_cellular_image(img[:,:,(i-1)], device=str(device)) for img in tiles]\n",
        "\n",
        "    #add results to tables\n",
        "    channelEntries = [pd.DataFrame( {'Channel':i, 'Tile':numtile+1, 'Count':get_num_cells(result[2]), 'Centroid_Coords': [get_centroids(result[2],cd)] }) for numtile, result in enumerate(results)]\n",
        "    SUMMARY_ARRAY = SUMMARY_ARRAY._append(channelEntries, ignore_index=True)\n",
        "\n",
        "    #calculate total number of cells for this channel\n",
        "    cells_per_channel.append( SUMMARY_ARRAY.loc[SUMMARY_ARRAY['Channel']==i, ['Count'] ].sum()[0] )\n",
        "    print(f\"Total number of cells in channel {i}: {cells_per_channel[-1]}\")\n",
        "\n",
        "  #add data to summary dataframe\n",
        "  SUMMARY_SMALL = pd.DataFrame({'Channel': channels, 'Total_Cell_Count': cells_per_channel})\n",
        "\n",
        "  #visualization with dots\n",
        "  fig = plt.figure(figsize=(40,40))\n",
        "\n",
        "  #for every tile\n",
        "  for i in range(len(tiles)):\n",
        "\n",
        "    #show tile\n",
        "    fig.add_subplot(3, 3, i+1)\n",
        "    plt.imshow(visualize_channels(np.transpose(tiles[i], (2,0,1)))*2, zorder=1)\n",
        "\n",
        "    #specify markers and colors for each channel\n",
        "    #currently supports 5 concurrent channels for cell counting\n",
        "    markers = ['o', '^', 's', 'x', 'p']\n",
        "    colors = ['red', 'orange', 'white', 'yellow', 'pink']\n",
        "\n",
        "    #draw dots\n",
        "    for j in range(len(channels)):\n",
        "      print(f\"Visualize{i}, {channels[j]}, {colors[j]}, {markers[j]}, xorder {j+2}\")\n",
        "      centroids = [v for val in SUMMARY_ARRAY.loc[ (SUMMARY_ARRAY['Tile']==i+1) & (SUMMARY_ARRAY['Channel']==channels[j]), ['Centroid_Coords']].to_numpy().flatten().tolist() for v in val]\n",
        "      for point in centroids: plt.scatter(point[0], point[1], c=colors[j], marker=markers[j], zorder=j+2)\n",
        "\n",
        "  return SUMMARY_ARRAY, SUMMARY_SMALL, fig\n"
      ],
      "metadata": {
        "id": "GouO_N03wo1s"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN"
      ],
      "metadata": {
        "id": "6IoLImXOIrh4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nGv3PSF7TPNu"
      },
      "outputs": [],
      "source": [
        "processed_img = None #global variable workaround to help with working with TIFF files without\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "  # Begin with a nice title and description for the page\n",
        "  gr.Markdown(\n",
        "      \"\"\"\n",
        "      # Welcome to CellSAM_XL\n",
        "      CellSAM XL is a tool to automate with quantitative analysis of large flourescent images.\n",
        "      Input: TIFF images.\n",
        "      Output: Visualization of identified cells. CSV of summary statistics (cell count and cell centroids) will be generated.\n",
        "      \"\"\")\n",
        "\n",
        "  # The first tab is for file uploads, processing, and channel seperation\n",
        "  with gr.Tab(\"Upload File\"):\n",
        "\n",
        "    # Message\n",
        "    gr.Markdown(\"\"\"\n",
        "        First, upload a tif image. Once uploaded, the image will be processed and displayed on the right hand side seperated by channel and as a composite image. Please use refer to these images to help determine which channels contain your cells of interest.\n",
        "        Please note: while multi-channel input is accepted, we do not currently support z-stacks images or videos.\n",
        "        \"\"\")\n",
        "\n",
        "    with gr.Row(): # create row to contain two columns\n",
        "\n",
        "      # file input and system message fields\n",
        "      with gr.Column(): #first column (left\n",
        "        input_img = gr.File()\n",
        "        channels_msg = gr.Textbox(label='System Message')\n",
        "\n",
        "      # output field for visualizations (display channels and composite image)\n",
        "      with gr.Column(): #second column\n",
        "\n",
        "        #individual channels\n",
        "        with gr.Row():\n",
        "          @gr.render(inputs=channels_msg) #whenever the channel message changes (aka new upload)\n",
        "          def show_channels(channels_msg):\n",
        "            if processed_img is not None:\n",
        "              for c in range(processed_img.shape[2]): #dynamically generate fields for each channel in rows of 2\n",
        "                gr.Image(processed_img[:,:,c], label=f\"Channel {c+1}\")  #display and label each channel appropriately\n",
        "\n",
        "        #also display the composite image underneath the channels\n",
        "        visualization = gr.Image(label='Composite Image')\n",
        "\n",
        "        #event listener\n",
        "        input_img.upload(fn=read_and_enhance_file, inputs=input_img, outputs=[channels_msg, visualization]) #event listener for image upload\n",
        "\n",
        "\n",
        "  #actual image processing tab\n",
        "  with gr.Tab(\"Process Image\"):\n",
        "\n",
        "    # Message\n",
        "    gr.Markdown(\"\"\"\n",
        "        In this part of the CellSAM XL pipeline, we enter some parameters to help the model perform well. Please select the channels that contains the cells that you wish to count and provide a rough diameter of your cells in pixles.\n",
        "        \"\"\")\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "\n",
        "      #input parameters for CellSAM_XL Pipeline\n",
        "      with gr.Column():\n",
        "\n",
        "        #set up channel selection field\n",
        "        channels_to_count = gr.CheckboxGroup(label=\"Select Channels to Count\", info=\"To show channel options, please first input an image in the Upload Files tab.\")\n",
        "\n",
        "        #Update channel selection field dynamically every time channel_msg is changed (new upload)\n",
        "        def update_channelslist(channels_to_count): return gr.CheckboxGroup(label=\"Select Channels to Count\", choices=range(1,processed_img.shape[2]+1))\n",
        "        channels_msg.change(update_channelslist, inputs=None, outputs=channels_to_count)\n",
        "\n",
        "        #set up cell size input field\n",
        "        cell_size = gr.Number(label=\"Cell Diameter (px)\", value=14.75)\n",
        "        start_button = gr.Button(\"Process Selected Channel(s)\")\n",
        "\n",
        "      #output field for results\n",
        "      with gr.Column():\n",
        "        visual = gr.Plot() #visualization field\n",
        "        small_df = gr.Dataframe(label=\"Summary\") #Short Summary dataframe\n",
        "        large_df = gr.Dataframe(label=\"Full Dataframe\") #Full Summary Dataframe\n",
        "\n",
        "      #event listener\n",
        "      start_button.click(fn=Run_CellSAM, inputs=[channels_to_count, cell_size], outputs=[large_df, small_df, visual])\n",
        "\n",
        "demo.launch(share=True)"
      ]
    }
  ]
}